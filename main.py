import asyncio
import json
import os
import uuid
import prefect
from prefect import task, Flow, Client
from prefect.storage import Docker
from prefect.run_configs import KubernetesRun
from malwaretl_stoq_transformer import transformer

DEBUG = False

if DEBUG is False:
    BASE_DIR = "/malware"
    OUTPUT_DIR = "/results"
else:
    BASE_DIR = "/Volumes/raw_malware"
    OUTPUT_DIR = "/tmp/"

SOURCE_DIRECTORIES = {
    "github-android-malware": ["github-android-malware/android-malware"],
    "malshare": ["malshare/2021", "malshare/2022"],
    "malwerk": ["malwerk/"],
    "mbazaar": ["mbazaar"],
    "theZoo": ["theZoo/theZoo/malware"],
    "urlhaus": ["urlhaus/2021", "urlhaus/2022"],
    "vxug": ["vxug/APT Collection",
             "vxug/Argus Collection",
             "vxug/Bazaar Collection",
             "vxug/Virusshare Collection"],
    "vxvault": ["vxvault"]
}


async def scan(stoq, archive_path, output_dir):
    with open(archive_path, "rb") as filehandle:
        results = await stoq.scan(content=filehandle.read())
        async for result in stoq.reconstruct_all_subresponses(results):
            try:
                sha256 = result['workers']['hash']['sha256']
                output_path = os.path.join(output_dir, sha256 + ".json")
                if not os.path.exists(output_path):
                    with open(output_path, "w") as outfile:
                        json.dump(result, outfile)
            except KeyError:
                print(f"failed to find hash in {result}")


@task
def prefect_task(job_id):
    logger = prefect.context.get("logger")
    logger.info(f"beginning task with job id {job_id}")
    analyze(job_id)
    logger.info(f"finished task with job id {job_id}")


def analyze(job_id):
    # for each collector, walk over its archive folder, hand each found file to stoq, but don't write results to
    # elasticsearch, write them to a flat .json file for each malware sample
    stoq = transformer.make_stoq("", stdout=True)
    loop = asyncio.new_event_loop()
    for source in SOURCE_DIRECTORIES:
        output_dir = os.path.join(OUTPUT_DIR, job_id, source)
        if not os.path.exists(output_dir):
            os.mkdir(output_dir)
        for directory in SOURCE_DIRECTORIES[source]:
            full_path = os.path.join(BASE_DIR, directory)
            for root, dirs, files in os.walk(full_path):
                for archive in files:
                    archive_path = os.path.join(root, archive)
                    if DEBUG is False:
                        loop.run_until_complete(scan(stoq, archive_path, output_dir))
                    else:
                        print(f"evaluating {archive_path}")


def main():
    job_id = str(uuid.uuid4())
    with Flow("malwareETL feature extractor",
              storage=Docker(registry_url="prefect-ui.g-clef.net:5000"),
              run_config=KubernetesRun(image="gclef/stoq-transformer:latest-prefect")) as flow:
        prefect_task(job_id)

    client = Client(api_server="http://prefect-ui.g-clef.net:4200/graphql")
    client.register(flow=flow, project_name="malware-feature-extraction")


if __name__ == "__main__":
    if DEBUG is False:
        main()
    else:
        analyze("testing")
