import asyncio
import json
import os

import prefect

from malwaretl_stoq_transformer import transformer

from job import config



async def scan(stoq, archive_path, output_dir):
    logger = prefect.context.get("logger")

    with open(archive_path, "rb") as filehandle:
        results = await stoq.scan(content=filehandle.read())
        async for result in stoq.reconstruct_all_subresponses(results):
            # not a super-fan of this, but I need to get the results to json quickly. That seems to involve
            # letting stoq do it's to-json-string magic, then pulling it back:
            string_result = str(result)
            decoded = json.loads(string_result)
            for entry in decoded.get("results", []):
                try:
                    sha256 = entry['payload_meta']['extra_data']['sha256']
                except KeyError:
                    try:
                        sha256 = entry['workers']['hash']['sha256']
                    except KeyError:
                        logger.info(f"failed to find hash in {entry}")
                        continue
                output_path = os.path.join(output_dir, sha256 + ".json")
                if not os.path.exists(output_path):
                    with open(output_path, "w") as outfile:
                        json.dump(entry, outfile)


@prefect.task(name="analyze")
def analyze(input_dir, output_dir, _dependency):
    # for each collector, walk over its archive folder, hand each found file to stoq, but don't write results to
    # elasticsearch, write them to a flat .json file for each malware sample
    loop = asyncio.new_event_loop()
    stoq = None
    logger = prefect.context.get("logger")
    for source in config.SOURCE_DIRECTORIES:
        logger.info(f"starting {source}")
        for directory in config.SOURCE_DIRECTORIES[source]:
            full_path = os.path.join(input_dir, directory)
            output_path = os.path.join(output_dir, directory)
            if not os.path.exists(output_path):
                os.makedirs(output_path, exist_ok=True)
            logger.info(f"looking at {full_path}")
            for root, dirs, files in os.walk(full_path):
                if config.DEBUG is False:
                    input_mode = transformer.InputMode.manual
                    output_mode = transformer.OutputMode.silent
                    stoq = transformer.make_stoq("", input_mode=input_mode, output_mode=output_mode)
                    # suppress the stoq worker logs. I wish the "failed to scan" logs weren't "error" level
                    stoq.log.disabled = True
                for archive in files:
                    archive_path = os.path.join(root, archive)
                    logger.info(f"looking at {archive}")
                    if config.DEBUG is False:
                        loop.run_until_complete(scan(stoq, archive_path, output_path))
                    else:
                        logger.info(f"evaluating {archive_path}")
